{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fala\r\n",
        "\r\n",
        "Cada vez mais, esperamos ser capazes de nos comunicar com sistemas de inteligência artificial (IA) falando com eles, frequentemente com a expectativa de receber uma resposta falada.\r\n",
        "\r\n",
        "![Um robô falando](./images/speech.jpg)\r\n",
        "\r\n",
        "O *Reconhecimento de fala* (um sistema de IA que interpreta a linguagem falada) e a *Síntese de fala* (um sistema de IA que gera uma resposta falada) são os principais componentes de uma solução de IA habilitada para a fala.\r\n",
        "\r\n",
        "## Criar um recurso dos Serviços Cognitivos\r\n",
        "\r\n",
        "Para criar um software que consiga interpretar fala audível e responder verbalmente, é possível usar o serviço cognitivo de **Fala**, que proporciona uma maneira simples de transformar linguagem falada em texto e vice-versa.\r\n",
        "\r\n",
        "Caso você ainda não tenha um, use as etapas a seguir para criar um recurso dos **Serviços Cognitivos** na sua assinatura do Azure:\r\n",
        "\r\n",
        "> **Observação**: Se você já tiver um recurso dos Serviços Cognitivos, abra a página de **Início Rápido** no portal do Azure e copie a respectiva chave e o ponto de extremidade para a célula abaixo. Caso contrário, siga as etapas abaixo para criar um.\r\n",
        "\r\n",
        "1. Em outra guia do navegador, abra o portal do Azure em https://portal.azure.com, entrando com sua conta Microsoft.\r\n",
        "2. Clique no botão **&#65291;Criar um recurso**, procure *Serviços Cognitivos* e crie um recurso dos **Serviços Cognitivos** com as configurações abaixo:\r\n",
        "    - **Assinatura**: *sua assinatura do Azure*.\r\n",
        "    - **Grupo de recursos**: *Selecione ou crie um grupo de recursos com um nome exclusivo*.\r\n",
        "    - **Região**: *Escolha qualquer região disponível*:\r\n",
        "    - **Nome**: *Insira um nome exclusivo*.\r\n",
        "    - **Tipo de preço**: S0\r\n",
        "    - **Confirmo que li e entendi os avisos**: Selecionado.\r\n",
        "3. Aguarde até que a implantação seja concluída. Depois, entre em seu recurso dos Serviços Cognitivos e, na página **Visão geral**, clique no link para gerenciar as chaves do serviço. Você precisará da chave e da localização para se conectar aos seus recursos dos Serviços Cognitivos em aplicativos clientes.\r\n",
        "\r\n",
        "### Obter a chave e a localização do seu recurso dos Serviços Cognitivos\r\n",
        "\r\n",
        "Para usar seu recurso dos Serviços Cognitivos, os aplicativos clientes precisam da chave de autenticação e da localização:\r\n",
        "\r\n",
        "1. No portal do Azure, na página **Chaves e ponto de extremidade** do seu recurso dos Serviços Cognitivos, copie a **Chave 1** do recurso e cole no código abaixo, substituindo **YOUR_COG_KEY**.\r\n",
        "2. Copie a **Localização** do recurso e cole no código abaixo, substituindo **YOUR_COG_LOCATION**.\r\n",
        ">**Observação**: continue na página **Chave e ponto de extremidade** e copie a **Localização** desta página (por exemplo: _westus_). _Não_ adicione espaços entre as palavras no campo Localização. \r\n",
        "3. Execute o código abaixo clicando no botão **Executar célula** (&#9655;) à esquerda."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cog_key = 'e09c79f3eeb54c2894df8b52f297e8f2'\r\n",
        "cog_location = 'https://cs-hsouza-01.cognitiveservices.azure.com/'\r\n",
        "\r\n",
        "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599695240794
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconhecimento de fala\r\n",
        "\r\n",
        "Suponha que você queira construir um sistema de automação residencial que aceite instruções faladas, como \"acender a luz\" ou \"apagar a luz\". Seu aplicativo precisará ser capaz de receber a entrada baseada em áudio (sua instrução falada) e interpretá-la, transformando o comando em um texto que possa ser analisado.\r\n",
        "\r\n",
        "Agora você está pronto para transcrever falas. A entrada pode vir de um **microfone** ou um **arquivo de áudio**. \r\n",
        "\r\n",
        "### Reconhecimento de fala com um microfone\r\n",
        "\r\n",
        "Vamos tentar primeiro com a entrada de um microfone. Execute a célula abaixo e, **imediatamente**, diga em voz alta **\"acender a luz\"**. Os recursos de reconhecimento de fala do serviço de Fala farão a transcrição do áudio. A saída deverá ser sua fala em forma de texto.\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "import IPython\r\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\r\n",
        "\r\n",
        "# Configure o reconhecimento de fala\r\n",
        "speech_config = SpeechConfig(cog_key, cog_location)\r\n",
        "\r\n",
        "# Faça os alunos dizerem \"acender as luzes\" \r\n",
        "speech_recognizer = SpeechRecognizer(speech_config)\r\n",
        "\r\n",
        "# Use uma chamada síncrona única para transcrever a fala\r\n",
        "speech = speech_recognizer.recognize_once()\r\n",
        "\r\n",
        "print(speech.text)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599695250434
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (!) Verificação\r\n",
        "\r\n",
        "Você conseguiu executar a célula e converter a fala em texto? Se a célula acima não produzir uma saída em texto (exemplo de saída: _Acender a luz_), experimente executá-la de novo e dizer \"acender a luz\" em voz alta **imediatamente**.\r\n",
        "\r\n",
        "### Reconhecimento de fala com um arquivo em áudio\r\n",
        "\r\n",
        "Se a célula acima não produzir uma saída em texto, seu microfone pode não estar configurado para aceitar a entrada. Em vez disso, execute a célula abaixo para ver o serviço de Reconhecimento de fala em ação com um **arquivo de áudio** em vez de uma **entrada do microfone**. \r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "from playsound import playsound\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
        "\n",
        "# Extraia o comando falado do arquivo de áudio\n",
        "file_name = 'light-on.wav'\n",
        "audio_file = os.path.join('data', 'speech', file_name)\n",
        "\n",
        "# Configure o reconhecimento de fala\n",
        "speech_config = SpeechConfig(cog_key, cog_location)\n",
        "audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n",
        "speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
        "\n",
        "# Use uma chamada síncrona única para transcrever a fala\n",
        "speech = speech_recognizer.recognize_once()\n",
        "\n",
        "# Reproduza o áudio e exiba o texto transcrito\n",
        "playsound(audio_file)\n",
        "print(speech.text)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sintetização de voz\r\n",
        "\r\n",
        "Agora você já viu como o serviço de Fala pode ser usado para transformar fala em texto, mas e o inverso? Como fazer a conversão de texto em fala?\r\n",
        "\r\n",
        "Bom, vamos supor que o seu sistema de automação residencial tenha interpretado o comando para acender a luz. Uma resposta apropriada seria reconhecer o comando verbalmente (além de realizar a tarefa solicitada!)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
        "%matplotlib inline\n",
        "\n",
        "# Obtenha o texto a ser falado\n",
        "response_text = 'Turning the light on.'\n",
        "\n",
        "# Configure a síntese de fala\n",
        "speech_config = SpeechConfig(cog_key, cog_location)\n",
        "speech_synthesizer = SpeechSynthesizer(speech_config)\n",
        "\n",
        "# Transforme texto em fala\n",
        "result = speech_synthesizer.speak_text(response_text)\n",
        "\n",
        "# Exiba uma imagem apropriada \n",
        "file_name = response_text + \"jpg\"\n",
        "img = Image.open(os.path.join(\"data\", \"speech\", file_name))\n",
        "plt.axis('off')\n",
        "plt. imshow(img)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599695261170
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimente alterar a variável **response_text** para *Apagando a luz.* (incluindo o ponto final) e executar a célula novamente para ouvir o resultado.\r\n",
        "\r\n",
        "## Saiba mais\r\n",
        "\r\n",
        "Neste notebook, você viu um exemplo muito simples do uso do serviço cognitivo de Fala. Saiba mais sobre a [Conversão de fala em texto](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-speech-to-text) e a [Conversão de texto em fala](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-text-to-speech) na documentação do serviço de Fala."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.8.5-final",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 32-bit",
      "metadata": {
        "interpreter": {
          "hash": "177429bd1865e7f7a0dbecbac90518c0d9641b1102b2e6c0df4b82dc948b5cb2"
        }
      }
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}